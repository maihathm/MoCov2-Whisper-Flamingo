{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "from config import get_config\n",
    "from datamodule.data_module import DataModule\n",
    "from models.av_net import AVNet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_deepcopyable(obj):\n",
    "    try:\n",
    "        copy.deepcopy(obj)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "class AVSRModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Convert config dict to a flat dictionary for hyperparameters,\n",
    "        # but only include items that can be deep-copied.\n",
    "        hparams = {}\n",
    "        for section, params in config.items():\n",
    "            if isinstance(params, dict):\n",
    "                for key, value in params.items():\n",
    "                    if is_deepcopyable(value):\n",
    "                        hparams[f\"{section}_{key}\"] = value\n",
    "            else:\n",
    "                if is_deepcopyable(params):\n",
    "                    hparams[section] = params\n",
    "        self.save_hyperparameters(hparams)\n",
    "        \n",
    "        # Model initialization\n",
    "        model_args = (\n",
    "            config[\"model\"][\"d_model\"],\n",
    "            config[\"model\"][\"n_heads\"],\n",
    "            config[\"model\"][\"n_layers\"],\n",
    "            config[\"model\"][\"pe_max_len\"],\n",
    "            config[\"model\"][\"fc_hidden_size\"],\n",
    "            config[\"model\"][\"dropout\"]\n",
    "        )\n",
    "        \n",
    "        self.model = AVNet(\n",
    "            modal=\"AV\",\n",
    "            MoCofile=os.path.join(os.getcwd(), config[\"data\"][\"moco_file\"]),\n",
    "            reqInpLen=config[\"model\"][\"required_input_length\"],\n",
    "            modelargs=model_args\n",
    "        )\n",
    "        \n",
    "        # MSE Loss for feature learning\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Prepare input data\n",
    "        input_data = (\n",
    "            batch[\"audios\"],\n",
    "            batch[\"audio_attention_mask\"],\n",
    "            batch[\"videos\"],\n",
    "            batch[\"video_attention_mask\"]\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        features = self.model(input_data)\n",
    "        \n",
    "        # Calculate MSE loss between audio and video features\n",
    "        loss = self.loss_fn(features, batch[\"audios\"])\n",
    "        \n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Log gate attention weights\n",
    "        if hasattr(self.model, 'fusion_module'):\n",
    "            for i, layer in enumerate(self.model.fusion_module.layers):\n",
    "                self.log(f'train_attn_gate_{i}', layer.attn_gate.item(), on_step=False, on_epoch=True)\n",
    "                self.log(f'train_ff_gate_{i}', layer.ff_gate.item(), on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Prepare input data\n",
    "        input_data = (\n",
    "            batch[\"audios\"],\n",
    "            batch[\"audio_attention_mask\"],\n",
    "            batch[\"videos\"],\n",
    "            batch[\"video_attention_mask\"]\n",
    "        )\n",
    "        \n",
    "        # Forward pass\n",
    "        features = self.model(input_data)\n",
    "        \n",
    "        # Calculate MSE loss between audio and video features\n",
    "        loss = self.loss_fn(features, batch[\"audios\"])\n",
    "        \n",
    "        # Log validation loss\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Calculate and log cosine similarity\n",
    "        cos_sim = F.cosine_similarity(features.mean(1), batch[\"audios\"].mean(1))\n",
    "        self.log('val_cosine_sim', cos_sim.mean(), on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Separate parameters for weight decay\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', 'gate']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': self.config[\"training\"][\"weight_decay\"]\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=0.0,  # Will be set by the scheduler\n",
    "            betas=(0.9, 0.98),\n",
    "            eps=1e-6\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler with warmup\n",
    "        num_training_steps = self.trainer.estimated_stepping_batches\n",
    "        num_warmup_steps = int(num_training_steps * 0.1)  # 10% warmup\n",
    "        \n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            return max(\n",
    "                0.0,\n",
    "                float(num_training_steps - current_step) / \n",
    "                float(max(1, num_training_steps - num_warmup_steps))\n",
    "            )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/maihathm/AVASR/MoCov2-Whisper-Flamingo/models/av_net.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.visual_model.load_state_dict(torch.load(MoCofile, map_location=\"cpu\"), strict=False)\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = get_config()\n",
    "\n",
    "# Initialize data module and model\n",
    "data_module = DataModule(config)\n",
    "model = AVSRModule(config)\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    # Model checkpointing\n",
    "    ModelCheckpoint(\n",
    "        dirpath=config[\"output\"][\"checkpoint_dir\"],\n",
    "        filename='avsr-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=config[\"output\"][\"save_top_k\"],\n",
    "        monitor=config[\"output\"][\"monitor\"],\n",
    "        mode=config[\"output\"][\"monitor_mode\"]\n",
    "    ),\n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor=config[\"output\"][\"monitor\"],\n",
    "        patience=config[\"training\"][\"early_stopping_patience\"],\n",
    "        mode=config[\"output\"][\"monitor_mode\"]\n",
    "    ),\n",
    "    # Learning rate monitor\n",
    "    pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/lightning_fabric/connector.py:558: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/amp.py:54: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Setup logger with detailed metrics\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=config[\"output\"][\"log_dir\"],\n",
    "    name='avsr_logs',\n",
    "    default_hp_metric=False\n",
    ")\n",
    "# Initialze trainer with improved settings\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config[\"training\"][\"epochs\"],\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    gradient_clip_val=config[\"training\"][\"gradient_clip_val\"],\n",
    "    accumulate_grad_batches=config[\"training\"].get(\"accumulate_grad_batches\", 1),\n",
    "    precision=16, \n",
    "    accelerator='auto',\n",
    "    devices=\"auto\",\n",
    "    strategy='ddp_notebook',\n",
    "    deterministic=False,\n",
    "    benchmark=True,\n",
    "    sync_batchnorm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [6,7]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with signal SIGSEGV",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:141\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    135\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    139\u001b[0m )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    144\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m/opt/miniconda/envs/whisper-flamingo/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:170\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m         name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<Unknown signal \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m-\u001b[39mexitcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    172\u001b[0m         error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    173\u001b[0m         error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    174\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    175\u001b[0m         signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    180\u001b[0m         error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    181\u001b[0m         error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    182\u001b[0m         exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    183\u001b[0m     )\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with signal SIGSEGV"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
